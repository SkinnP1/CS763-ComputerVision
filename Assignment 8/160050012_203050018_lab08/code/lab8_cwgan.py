# -*- coding: utf-8 -*-
"""lab8_cwgan_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19_D2qF-QH9aoHaZsbIjra_SFC3eg1p-D
"""

#!/usr/bin/env python
# coding: utf-8

#
# # CS 763 - Lab 8 (WGAN-GPs) Part-B

from torch.autograd import grad
from tqdm import tqdm
import time
import sys
import os
import PIL
from torch.nn import functional as F
from torch import nn, optim
from torchvision import transforms, models, datasets
import matplotlib.pyplot as plt
import torchvision
import random
import torch
import numpy as np

from google.colab import drive
drive.mount('/content/drive')

# Check for GPU specs
get_ipython().system('nvidia-smi')


# device set to cuda if GPU available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Function to fix the seed for randomisation


def fix_seed(seed=0):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


fix_seed(763)

# We will use the Fashion-MNIST dataset for this task
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean=(0.5), std=(0.5))])

trainset = torchvision.datasets.FashionMNIST(
    './data', download=True, train=True, transform=transform)
testset = torchvision.datasets.FashionMNIST(
    './data', download=True, train=False, transform=transform)

# dataloaders
trainloader = torch.utils.data.DataLoader(
    trainset, batch_size=128, num_workers=2, shuffle=True)
testloader = torch.utils.data.DataLoader(
    trainset, batch_size=128, num_workers=2)


# Visualising the dataset
labeldic = {}
labeldic[0] = 'T-shirt/top'
labeldic[1] = 'Trouser'
labeldic[2] = 'Pullover'
labeldic[3] = 'Dress'
labeldic[4] = 'Coat'
labeldic[5] = 'Sandal'
labeldic[6] = 'Shirt'
labeldic[7] = 'Sneaker'
labeldic[8] = 'Bag'
labeldic[9] = 'Ankle boot'
print('Lenth of Trainset = {}'.format(len(trainset)))
print('Lenth of Testset = {}'.format(len(testset)))
print('Dimensions of each image = ({},{})'.format(
    trainset[0][0].shape[1], trainset[0][0].shape[2]))
fig, axs = plt.subplots(4, 10, sharey=True, figsize=(20, 10))
for i in range(40):
    axs[i//10, i % 10].imshow(trainset[i][0].squeeze(), cmap='gray')
    axs[i//10, i % 10].set_title('{}'.format(labeldic[trainset[i][1]]))
plt.suptitle('Fashion MNIST Train Images')
plt.show()

# Generator and Discriminator Architecures

# The generator takes a noise vector of size (B,100), where B is the batch size and produces a 28x28 image
class Generator(nn.Module):
    def __init__(self):
        super(Generator, self).__init__()
        ## TODO ##
        noise_dim = 110 #input to generator layer 1 i.e. inchannel
        feat_map_dim = 32 #output of layer 1 i.e. outchannel
        kernel_size = 4
        stride = 2
        padding = 1
        channels = 1
        self.one_hot_vector = torch.eye(10,10)

        self.network = nn.Sequential(
            # 110 x 1 x 1
            nn.ConvTranspose2d(noise_dim, feat_map_dim * 8, kernel_size, stride, padding, bias=False),
            nn.BatchNorm2d(feat_map_dim * 8),
            nn.ReLU(True),
            # 32*8 x 2 x 2
            nn.ConvTranspose2d(feat_map_dim * 8, feat_map_dim * 4, kernel_size, stride, padding, bias=False),
            nn.BatchNorm2d(feat_map_dim * 4),
            nn.ReLU(True),
            # 32*4 x 4 x 4
            nn.ConvTranspose2d(feat_map_dim * 4, feat_map_dim * 2, kernel_size, stride, padding, bias=False),
            nn.BatchNorm2d(feat_map_dim * 2),
            nn.ReLU(True),
            # 32*2 x 8 x 8
            nn.ConvTranspose2d(feat_map_dim * 2, feat_map_dim, kernel_size, stride, padding, bias=False),
            nn.BatchNorm2d(feat_map_dim),
            nn.ReLU(True),
            # 32 x 16 x 16
            nn.ConvTranspose2d(feat_map_dim , channels, 2, 2, 2, bias=False),
            nn.Tanh()
            # 1 x 28 x 28
        )
        ## TODO ##

    # forward pass
    def forward(self, x, labels):
        ## TODO ##
        input = torch.zeros(x.size(0), 110, 1, 1).to(device)
        for j in range(x.size(0)):
            l = int(labels[j].item())
            oneHot = self.one_hot_vector[l]
            b = oneHot.reshape(10,1,1)
            input[j,:100,:,:] = x[j]
            input[j,100:,:,:] = b
        return self.network(input)
        ## TODO ##

class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        ## TODO ##
        noise_dim = 110 #input to generator layer 1 i.e. inchannel
        feat_map_dim = 32 #output of layer 1 i.e. outchannel
        kernel_size = 4
        stride = 2
        padding = 1
        channels = 11
        self.one_hot_vector = torch.eye(10,10)

        self.network = nn.Sequential(
            # 11 x 28 x 28
            nn.Conv2d(channels, feat_map_dim, 2, stride, 2, bias=False),
            nn.LeakyReLU(0.02,inplace=True),
            # 32 x 16 x 16
            nn.Conv2d(feat_map_dim, feat_map_dim * 2 , kernel_size, stride, padding, bias=False),
            nn.BatchNorm2d(feat_map_dim * 2),
            nn.LeakyReLU(0.02,inplace=True),
            # 32*2 x 8 x 8
            nn.Conv2d(feat_map_dim * 2, feat_map_dim * 4 , kernel_size, stride, padding, bias=False),
            nn.BatchNorm2d(feat_map_dim * 4),
            nn.LeakyReLU(0.02,inplace=True),
            # 32*4 x 4 x 4
            nn.Conv2d(feat_map_dim * 4, feat_map_dim * 8 , kernel_size, stride, padding, bias=False),
            nn.BatchNorm2d(feat_map_dim * 8),
            nn.LeakyReLU(0.02,inplace=True),
            # 32*8 x 2 x 2
            nn.Conv2d(feat_map_dim * 8, 1, kernel_size, stride, padding),
            # 1 x 1 x 1
        )
        ## TODO ##

    # forward pass
    def forward(self, x, labels):
        ## TODO ##
        processed = torch.zeros(x.size(0),11,28,28).to(device)
        for j in range(x.size(0)):
            img = x[j]
            l = labels[j]
            oneHot = self.one_hot_vector[l].reshape(10,1)
            one_hot_vec = oneHot.repeat(1,28)
            one_hot_vec = one_hot_vec.unsqueeze(-1).expand(10,28,28)
            processed[j,1:,:,:] = one_hot_vec
            processed[j,0,:,:] = img.reshape(28,28)
        return self.network(processed)
        ## TODO ##

# Create the models
G = Generator().to(device)
D = Discriminator().to(device)

print(G)
print(D)

# Define the Optimizer
## TODO ##
optimizer_g = optim.Adam(G.parameters(), lr = 0.0002,betas = (0.5,0.999))
optimizer_d = optim.Adam(D.parameters(), lr = 0.0002)
## TODO ##

# Define the Washerstein Loss functions
lamda = 10


def get_WLoss_generator(fake_images, labels):
    prediction_on_fake = D(fake_images,labels).view(-1)
    return -torch.mean(prediction_on_fake)

def get_WLoss_discriminator(fake_images, true_images, labels):

    prediction_on_fake = D(fake_images,labels).view(-1)
    prediction_on_real = D(true_images,labels).view(-1)

    scoreFake = torch.mean(prediction_on_fake)
    scoreReal = torch.mean(prediction_on_real)
    penalty = get_gradient_regularisation(fake_images, true_images, labels)
    return (scoreFake-scoreReal+lamda*(penalty))

def get_gradient_regularisation(fake_images, true_images, labels):
    grads = get_grad(fake_images.to(device), true_images.to(device), labels)
    return torch.mean((torch.norm(grads)-1)**2)

def get_grad(fake_images, true_images, labels):
    epsilon = torch.FloatTensor(np.random.uniform(
        size=(fake_images.shape[0], 1, 1, 1))).to(device)
    xbar = epsilon*true_images + (1-epsilon)*fake_images
    xbar.requires_grad_()
    lipschitz_grad = grad(
        outputs=D(xbar, labels).sum(),
        inputs=xbar,
        create_graph=True,
        retain_graph=True)[0]
    return lipschitz_grad.view(xbar.shape[0], -1)

# Train Function
# returns the total loss for the epoch


def train(epoch):
    loss_acc_g = 0
    loss_acc_d = 0

    for i, (images, labels) in tqdm(enumerate(trainloader), desc="Training for epoch {}".format(epoch), total=len(trainloader)):
        # Discriminator Training
        optimizer_d.zero_grad()  
        ## TODO ##
       #### Generator Input
        noise = torch.randn(images.shape[0],100, 1, 1).to(device)
        fake_samples = G(noise.to(device),labels)

        loss_d = get_WLoss_discriminator(fake_samples,images,labels) 
        # loss_d.requires_grad = True
        loss_d.backward(retain_graph=True)
        ## TODO ##
        optimizer_d.step()

        if i % 5 == 0:
            # Generator training
            optimizer_g.zero_grad()
            ## TODO ##
            loss_g = get_WLoss_generator(fake_samples,labels)
            # loss_g.requires_grad = True
            loss_g.backward(retain_graph=True)
            ## TODO ##
            optimizer_g.step()
            loss_acc_g += loss_g.item()
        loss_acc_d += loss_d.item()

    return (loss_acc_g/(i/5)+loss_acc_d/i), loss_acc_g/(i/5), loss_acc_d/i

# Functions to save and load checkpoints
# Enter the path on your shared drive where the checkpoint is to be saved
## TODO ##
# For example path_to_checkpoint = '/content/drive/Shareddrives/CS763Lab8/'
path_to_checkpoint = "/content/drive/Shareddrives/CV LAB08 GANs"
## TODO ##


def save_checkpoint(e):
    if not os.path.isdir(path_to_checkpoint):
        os.mkdir(path_to_checkpoint)
    torch.save({'e': e, 'gen': G.state_dict(), 'disc': D.state_dict(), 'optim_d': optimizer_d.state_dict(
    ), 'optim_g': optimizer_g.state_dict()}, os.path.join(path_to_checkpoint, 'checkpoint_cwgan.pth'))


def load_checkpoint():
    if not os.path.isfile(os.path.join(path_to_checkpoint, 'checkpoint_cwgan.pth')):
        return -1
    dic = torch.load(os.path.join(path_to_checkpoint, 'checkpoint_cwgan.pth'))
    G.load_state_dict(dic['gen'])
    D.load_state_dict(dic['disc'])
    optimizer_d.load_state_dict(dic['optim_d'])
    optimizer_g.load_state_dict(dic['optim_g'])
    return dic['e']

# The generate functions are defined here
# Should return a nx28x28 numpy array


def generate(n, ci):
    pass
    ## TODO ##
    batch_size = n
    noise_dim = 100
    noise = torch.randn(batch_size, noise_dim, 1, 1, device=device)
    # print(ci)
    label = torch.ones(batch_size) * ci
    # print(label)
    # print(label.shape)
    return G(noise,label).detach().cpu()
    ## TODO ##

## TODO ##
NUM_EPOCHS = 15
## TODO ##
losses_g = []
losses_d = []

# Resume training
offset = load_checkpoint()
if os.path.exists('images'):
    if offset == -1:
        os.system('rm -rf images')
        os.mkdir('images')
else:
    os.mkdir('images')


for e in range(NUM_EPOCHS):
    if e <= offset:
        continue
    loss, loss_g, loss_d = train(e)
    losses_g.append(loss_g)
    losses_d.append(loss_d)
    G.eval()
    li = torch.randint(0, 10, (1,))[0].item()
    g_output = generate(1, li)[0]
    G.train()
    plt.imshow(g_output.reshape(28, 28), cmap='gray')
    plt.title('Category: {}'.format(labeldic[li]))
    plt.savefig('./images/{}.png'.format(e))
    print('loss_g = {}, loss_d = {}\n'.format(loss_g, loss_d))
    save_checkpoint(e)

# Test script
load_checkpoint()
with torch.no_grad():
    G.eval()
    fig, axs = plt.subplots(4, 10, sharey=True, figsize=(20, 10))
    for i in range(40):
        fake_image = generate(1, i % 10)[0]
        axs[i//10, i % 10].imshow(fake_image.reshape(28, 28), cmap='gray')
        axs[i//10, i % 10].set_title('{}'.format(labeldic[i % 10]))
    plt.suptitle('Fashion MNIST Generated Images')
    plt.show()

